{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6620c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions \n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import col, split, element_at, when\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "#To decode the geohashes, the module pygeohrash will be used.\n",
    "!pip install pygeohash #uncomment to install the module\n",
    "import pygeohash as pgh\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c65da7",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390e466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "host_ip = \"118.139.86.28\"\n",
    "\n",
    "topics = \"climateTopic, hotspotTopic\"\n",
    "\n",
    "#create spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming Fires Data')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#set up MongoDB\n",
    "client = MongoClient(host_ip, 27017)\n",
    "db = client.fit3182_assignment_db\n",
    "\n",
    "#storing in new collections\n",
    "climate = db.climate_stream\n",
    "hotspot = db.hotspot_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3906457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set up spark readStream\n",
    "kafka_sdf = (\n",
    "    spark.readStream\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topics)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59072314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#only retrieve value from the stream output\n",
    "fire_sdf = kafka_sdf.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926abd6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def average_hotspot(hpList):\n",
    "    \"\"\"\n",
    "    hpList: list of hotspots with the same geohash5 location\n",
    "    \n",
    "    This function is to calculate the average values of each fields for data \n",
    "    collected from AQUA and TERRA that are from the same location.\n",
    "    \"\"\"\n",
    "    datetimes = []\n",
    "    latitude_lst = []\n",
    "    longitude_lst = []\n",
    "    confidence_lst = []\n",
    "    surfaceTemp_lst = []\n",
    "    \n",
    "    for hp in hpList:\n",
    "        datetimes.append(hp[\"datetime\"])\n",
    "        latitude_lst.append(hp[\"latitude\"])\n",
    "        longitude_lst.append(hp[\"longitude\"])\n",
    "        confidence_lst.append(hp[\"confidence\"])\n",
    "        surfaceTemp_lst.append(hp[\"surface_temperature\"])\n",
    "    \n",
    "    meanDT = (np.array(datetimes, dtype='datetime64[s]')\n",
    "           .view('i8')\n",
    "           .mean()\n",
    "           .astype('datetime64[s]'))\n",
    "    \n",
    "    doc = {}\n",
    "    doc[\"date\"] = hpList[0][\"date\"]\n",
    "    doc[\"datetime\"] = print(meanDT, 'HEREEE')\n",
    "    doc[\"latitude\"] = (sum(latitude_lst))/(len(latitude_lst))\n",
    "    doc[\"longitude\"] = (sum(longitude_lst))/(len(longitude_lst))\n",
    "    doc[\"confidence\"] = (sum(confidence_lst))/(len(confidence_lst))\n",
    "    doc[\"surface_temperature\"] = (sum(surfaceTemp_lst))/(len(surfaceTemp_lst))\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b19d014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def process_batch(df, epoch_id):\n",
    "    data = df.collect()\n",
    "\n",
    "    #if the stream is empty\n",
    "    if len(data) == 0:\n",
    "        return \n",
    "    \n",
    "    climate_row = json.loads(data[0].value)\n",
    "    \n",
    "    #check that the first stream is a climate data\n",
    "    if climate_row[\"producer_info\"] != \"climate\":\n",
    "        return\n",
    "    \n",
    "    #make our climate doc \n",
    "    climate_record = {}\n",
    "    climate_record[\"station\"] = int(\"468802\")\n",
    "    climate_record[\"date\"] = climate_row[\"date\"]\n",
    "    climate_record[\"latitude\"] = climate_row[\"latitude\"]\n",
    "    climate_record[\"longitude\"] = climate_row[\"longitude\"]\n",
    "    climate_record[\"air_temperature\"] = climate_row[\"air_temperature\"]\n",
    "    climate_record[\"relative_humidity\"] = climate_row[\"relative_humidity\"]\n",
    "    climate_record[\"windspeed_knots\"] = climate_row[\"windspeed_knots\"]\n",
    "    climate_record[\"max_wind_speed\"] = climate_row[\"max_wind_speed\"]\n",
    "    climate_record[\"precipitation\"] = climate_row[\"precipitation\"]\n",
    "    climate_record[\"prec_val\"] = climate_row[\"prec_val\"]\n",
    "    climate_record[\"prec_type\"] = climate_row[\"prec_type\"]\n",
    "    climate_record[\"GHI\"] = climate_row[\"GHI\"]\n",
    "    \n",
    "    \n",
    "    #check that climate data isn't the only one that came in the stream\n",
    "    if len(data) > 1:\n",
    "        climate_lat = climate_row[\"latitude\"]\n",
    "        climate_long = climate_row[\"longitude\"]\n",
    "        climate_geohash = pgh.encode(climate_lat, climate_long, precision = 3)\n",
    "\n",
    "        climate_date = climate_row[\"date\"][:10]\n",
    "                    \n",
    "        hotspotList = []\n",
    "        hotspot_geo = []\n",
    "        \n",
    "        #examine the other docs\n",
    "        for i in range(1, len(data)):\n",
    "            currDoc = json.loads(data[i].value)\n",
    "\n",
    "            #find the location of the current hotspot data we are examining\n",
    "            temp_lat = currDoc[\"latitude\"]\n",
    "            temp_long = currDoc[\"longitude\"]\n",
    "            temp_geo = pgh.encode(temp_lat, temp_long, precision = 3)\n",
    "            \n",
    "\n",
    "            if currDoc[\"producer_info\"] != \"climate\":  \n",
    "            \n",
    "                temp_dt = climate_date + currDoc[\"datetime\"][10:]\n",
    "                \n",
    "                #compare that hotspot and climate are in the same location\n",
    "                if temp_geo == climate_geohash:\n",
    "\n",
    "                    hotspot_doc = {} \n",
    "                    hotspot_doc[\"date\"] = climate_row[\"date\"]\n",
    "                    hotspot_doc[\"datetime\"] = temp_dt\n",
    "                    hotspot_doc[\"latitude\"] = currDoc[\"latitude\"]\n",
    "                    hotspot_doc[\"longitude\"] = currDoc[\"longitude\"]\n",
    "                    hotspot_doc[\"confidence\"] = currDoc[\"confidence\"]\n",
    "                    hotspot_doc[\"surface_temperature\"] = currDoc[\"surface_temperature\"]\n",
    "                    \n",
    "                    #check if the fire is natural or other\n",
    "                    if climate_record[\"air_temperature\"] > 20 \\\n",
    "                        and climate_record[\"GHI\"]>180:\n",
    "                        \n",
    "                        hotspot_doc[\"fire_cause\"]=\"natural\"\n",
    "                        \n",
    "                    else:\n",
    "                        hotspot_doc[\"fire_cause\"]=\"other\"\n",
    "\n",
    "                    #find the geohash5 of the hotspot data\n",
    "                    hotspotList.append(hotspot_doc)\n",
    "                    hotspot_geo.append(pgh.encode(temp_lat,temp_long, precision=5))\n",
    "\n",
    "                    \n",
    "        #retrieve a counter of the geohash5 encodings\n",
    "        #if count is more than 1, there are multiple records\n",
    "        #in the same location\n",
    "        geoCodeCount = Counter(hotspot_geo)\n",
    "        \n",
    "        #record the documents with same geohash5 encodings\n",
    "        duplicates = []\n",
    "        dup_codes = list([code for code in geoCodeCount if geoCodeCount[code]>1])\n",
    "        \n",
    "        for geocode in dup_codes:\n",
    "            for dupIndex in range(len(hotspot_geo)):\n",
    "                if hotspot_geo[dupIndex] == geocode:\n",
    "                    \n",
    "                    #append it to duplicates\n",
    "                    duplicates.append(hotspotList[dupIndex])\n",
    "                    \n",
    "                    #remove the hotspot_doc from the hotspot array\n",
    "                    hotspotList.remove(hotspotList[dupIndex])\n",
    "                    \n",
    "                    #retrieve an average of the hotspot data values\n",
    "                    newDoc = average_hotspot(duplicates)\n",
    "                    hotspotList.append(newDoc)\n",
    "                    \n",
    "        \n",
    "        fires = []   \n",
    "        for h in hotspotList:\n",
    "            #insert hotspot document into mongo\n",
    "            hotspot.insert_one(h)\n",
    "\n",
    "            #array appends all the corresponding hotspot id\n",
    "            fires.append(h[\"_id\"])\n",
    "\n",
    "        climate_record[\"hotspots\"] = fires\n",
    "        \n",
    "        #insert climate document into mongo\n",
    "        climate.insert_one(climate_record)\n",
    "        \n",
    "    print(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9c09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = (fire_sdf.writeStream.format(\"console\")\n",
    "         .option(\"checkpointLocation\", \"./fires_sdf_checkpoints\")\n",
    "         .outputMode('complete')\n",
    "         .trigger(processingTime = '10 second')\n",
    "         .foreachBatch(process_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca88aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    query = writer.start()\n",
    "    query.awaitTermination()  \n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
